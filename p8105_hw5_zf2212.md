p8105\_hw5\_zf2212
================
Catherine
11/2/2018

### Problem 1

This problem is based on the data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

#### 1.1

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.

``` r
# creat file names list with path
file_list = tibble(
  file_name = list.files(path = "./data")) %>% 
  mutate(path = str_c("./data/", file_name)) # rename the variable

# read in data and tidy
data_raw = file_list %>% 
  mutate(path, path = map(path, read_csv)) %>% # map and save the result
  unnest() 
data_tidy = data_raw %>% 
  separate(file_name, into = c("arm", "id"), sep = "_") %>% 
  mutate(arm, arm = recode(arm, 
                           con = "control",
                           exp = "experimental"), # make values clear
         id = str_replace(id, ".csv", "")) %>% 
  select(id, arm, everything())

# skimr::skim(data_tidy)
```

**Description**

There are 20 observations and 10 columns in the dataset and no missing values. **Variables** include id, arm, week\_1, week\_2, week\_3, week\_4, week\_5, week\_6, week\_7, week\_8. Other than `id` and `arm`, all variables are class `numeric`.

#### 1.2

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

``` r
# prepare the data
data_plot = data_tidy %>% 
  gather(key = "week", value = "observation", week_1:week_8) %>% # gather the data for plot
  mutate(week, week = as.numeric(str_replace(week, "week_",""))) # convert week into number
# make the plot
data_plot %>% 
  ggplot(aes(x = week, y = observation, color = id)) +
  geom_point(alpha = .6, size = .8) +
  geom_line(alpha = .6) +
  stat_summary(fun.y = mean, color = "black", 
               position = position_dodge(0.75),
               geom = "line",
               size = .8,
               alpha = .6) +
  facet_grid(~arm) + 
  scale_x_discrete(limits = c(1:8)) + # break in 8 weeks
  labs(title = "Spaghetti Plot of Control and Experimental Arms",
       x = "Week",
       y = "Observation",
       caption = "The black lines show the mean of each group") +
  theme_bw() + # set names and themes
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5, face = 'bold')) +
  guides(col = guide_legend(nrow = 1))
```

<img src="p8105_hw5_zf2212_files/figure-markdown_github/spaghetti plot-1.png" width="90%" />

**Comment**

There is a growing trend for participants in the experimental group over 8 weeks of study.

There is no significant trend for participants in the control group.

Most of the participants in the control group were observed lower values compared to experimental group.

### Problem 2

This problem is based on the data gathered by The Washington Post about homicides in 50 large U.S. cities over the past decade.

#### 2.1

Describe the raw data. Create a city\_state variable and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

``` r
# import data 
homicide_raw = read_csv("./data_homicides/homicide_data.csv", col_types = "cccccccccddc")
# look at the raw dataset
str(homicide_raw)
```

**Description**

The raw data includes 52179 observations and 12 variables. The dataset includes the information of 52179 criminal homicides cases over the past decade in 51 distincts cities. **Variables** include uid, reported\_date, victim\_last, victim\_first, victim\_race, victim\_age, victim\_sex, city, state, lat, lon, disposition. Other than `lat` and `lon`, all variables are class `character`. `victim_race` includes 6 distinct races. `disposition` includes 3 kinds of disposition for cases.

``` r
# create new variable
homicide_data = homicide_raw %>% 
  mutate(city_state = str_c(city,state, sep = ", ")) %>% 
  select(uid, reported_date, city_state, everything()) 
# summarize the homicide data
homicide_sum = homicide_data %>% 
  group_by(city_state) %>% 
  summarize(total_homicides = n(),
            total_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))
```

#### 2.2

Estimate the proportion of homicides that are unsolved for the city of Baltimore, MD

``` r
# filter the baltimore data
bal_data = homicide_sum %>% 
  filter(city_state == "Baltimore, MD")
# prop test for baltimore
tb_bal = prop.test(bal_data$total_unsolved, bal_data$total_homicides) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high) # pull estimated proportion and CI
knitr::kable(tb_bal)
```

|   estimate|   conf.low|  conf.high|
|----------:|----------:|----------:|
|  0.6455607|  0.6275625|  0.6631599|

For the city of **Baltimore, MD**, the estimated proportion is 0.6455607, the confidence intervals is (0.6275625, 0.6631599).

#### 2.3

Extract both the proportion of unsolved homicides and the confidence interval for all cities.

``` r
city_prop_test = function(x, n) {
  prop.test(x, n) %>% 
    broom::tidy() %>% 
    select(estimate, conf.low, conf.high) %>% 
    round(3)
    
}
```

``` r
city_prop = homicide_sum %>% 
  # map the data by the function
  mutate(prop = map2(.x = total_unsolved, .y = total_homicides, ~city_prop_test(.x, .y))) %>% 
  unnest() %>% 
  select(city_state, estimate, conf.low, conf.high)
```

    ## Warning in prop.test(x, n): Chi-squared approximation may be incorrect

``` r
knitr::kable(city_prop)
```

| city\_state        |  estimate|  conf.low|  conf.high|
|:-------------------|---------:|---------:|----------:|
| Albuquerque, NM    |     0.386|     0.337|      0.438|
| Atlanta, GA        |     0.383|     0.353|      0.415|
| Baltimore, MD      |     0.646|     0.628|      0.663|
| Baton Rouge, LA    |     0.462|     0.414|      0.511|
| Birmingham, AL     |     0.434|     0.399|      0.469|
| Boston, MA         |     0.505|     0.465|      0.545|
| Buffalo, NY        |     0.612|     0.569|      0.654|
| Charlotte, NC      |     0.300|     0.266|      0.336|
| Chicago, IL        |     0.736|     0.724|      0.747|
| Cincinnati, OH     |     0.445|     0.408|      0.483|
| Columbus, OH       |     0.530|     0.500|      0.560|
| Dallas, TX         |     0.481|     0.456|      0.506|
| Denver, CO         |     0.542|     0.485|      0.598|
| Detroit, MI        |     0.588|     0.569|      0.608|
| Durham, NC         |     0.366|     0.310|      0.426|
| Fort Worth, TX     |     0.464|     0.422|      0.507|
| Fresno, CA         |     0.347|     0.305|      0.391|
| Houston, TX        |     0.507|     0.489|      0.526|
| Indianapolis, IN   |     0.449|     0.422|      0.477|
| Jacksonville, FL   |     0.511|     0.482|      0.540|
| Kansas City, MO    |     0.408|     0.380|      0.437|
| Las Vegas, NV      |     0.414|     0.388|      0.441|
| Long Beach, CA     |     0.413|     0.363|      0.464|
| Los Angeles, CA    |     0.490|     0.469|      0.511|
| Louisville, KY     |     0.453|     0.412|      0.495|
| Memphis, TN        |     0.319|     0.296|      0.343|
| Miami, FL          |     0.605|     0.569|      0.640|
| Milwaukee, wI      |     0.361|     0.333|      0.391|
| Minneapolis, MN    |     0.511|     0.459|      0.563|
| Nashville, TN      |     0.362|     0.329|      0.398|
| New Orleans, LA    |     0.649|     0.623|      0.673|
| New York, NY       |     0.388|     0.349|      0.427|
| Oakland, CA        |     0.536|     0.504|      0.569|
| Oklahoma City, OK  |     0.485|     0.447|      0.524|
| Omaha, NE          |     0.413|     0.365|      0.463|
| Philadelphia, PA   |     0.448|     0.430|      0.466|
| Phoenix, AZ        |     0.551|     0.518|      0.584|
| Pittsburgh, PA     |     0.534|     0.494|      0.573|
| Richmond, VA       |     0.263|     0.223|      0.308|
| Sacramento, CA     |     0.370|     0.321|      0.421|
| San Antonio, TX    |     0.429|     0.395|      0.463|
| San Bernardino, CA |     0.618|     0.558|      0.675|
| San Diego, CA      |     0.380|     0.335|      0.426|
| San Francisco, CA  |     0.507|     0.468|      0.545|
| Savannah, GA       |     0.467|     0.404|      0.532|
| St. Louis, MO      |     0.540|     0.515|      0.564|
| Stockton, CA       |     0.599|     0.552|      0.645|
| Tampa, FL          |     0.457|     0.388|      0.527|
| Tulsa, AL          |     0.000|     0.000|      0.945|
| Tulsa, OK          |     0.331|     0.293|      0.371|
| Washington, DC     |     0.438|     0.411|      0.465|

#### 2.4

Create a plot that shows the estimates and CIs for each city. Organize cities according to the proportion of unsolved homicides.

``` r
city_prop %>% 
  # organize the city according to proportion
  mutate(city_state = forcats::fct_reorder(city_state, estimate, .desc = T)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point(color = "red") + # plot the estimate
  geom_errorbar(aes(x = city_state, ymin = conf.low, ymax = conf.high)) + # plot the intervals
  theme(axis.text.x = element_text(angle = 90), plot.title = element_text(hjust = 0.5, face = 'bold')) +
  labs(title = "Estimates and CIs of the Unsolved Homicides Proportion in 50 American Cities",
       x = "City",
       y = "Proportion",
       caption = "Data from The Washington Post") 
```

<img src="p8105_hw5_zf2212_files/figure-markdown_github/city plot-1.png" width="90%" />

The plot shows the estimates and CIs for 51 American cities. Most of the cities hava an unsolved homicide proportion over 25%. It reflects the problem that there are many areas across the country where murder is common but arrests are rare.
